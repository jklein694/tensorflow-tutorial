{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to TensorFlow\n",
    "\n",
    "### Learning Objectives\n",
    "- Overview of TensorFlow\n",
    "- Graphs and Sessions\n",
    "- Operations and Variables\n",
    "- Using TensorBoard\n",
    "- Models in TensorFlow\n",
    "- Neural Networks in TensorFlow\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://www.tensorflow.org/_static/images/tensorflow/logo.png)\n",
    "\n",
    "### What is TensorFlow\n",
    "- In 2011, Google Brain built DistBelief, the precursor of TensorFlow, as a propreitart machine learning system specifically for deep learning neural networks. \n",
    "- DistBelief quickly gained in popularity, and after a few upgrades was renamed to TensorFlow.\n",
    "- While DistBelief was very successful, it had some limitations. It was narrowly targeted to neural networks.\n",
    "- Geoffrey Hinton, the team lead for the development of TensorFlow, implemented generalized backpropagation and other improvements which allowed generation of neural networks with substantially higher accuracy, for instance a 25% reduction in errors in speech recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://pbs.twimg.com/media/C6k0JvBVwAEXJfZ.jpg)\n",
    "\n",
    "### Why TensorFlow?\n",
    "- Portability: deploy computation to one or more CPUs or GPUs in a desktop, server, or mobile device with a single API.\n",
    "- TensorBoard (Which is awesome)\n",
    "- TensorFlow was built from the ground up to be fast, portable, and ready for production service.\n",
    "- Faster and more customizeable than Keras, but slightly more complicated.\n",
    "- **TensorFlow is yours.**\n",
    "    - TensorFlow is an open-sourced standalone library and associated tools, tutorials, and examples with the Apache 2.0 license so you’re free to use whenever.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://mancap314.github.io/images/tensorflow.png)\n",
    "\n",
    "# First - Build Phase: Build your tensor flow graph.\n",
    "### -- A graph defines the computation. It doesn’t compute anything, it doesn’t hold any values, it just defines the operations that you specified in your code.\n",
    "\n",
    "# Second - Execution Phase:  A Session to run all of your computations at once.\n",
    "###    -- A session allows to execute graphs or part of graphs. It allocates resources (on one or more machines) for that and holds the actual values of intermediate results and variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is a Tensor?\n",
    "## An n-dimensional array\n",
    "### - 0-d tensor: scalar (number)\n",
    "### - 1-d tensor: vector\n",
    "### - 2-d tensor: matrix\n",
    "\n",
    "![](https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/Images/scalar-vector-matrix.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What do you think is going to happend when I run this cell?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-08T15:41:44.649446Z",
     "start_time": "2017-11-08T15:41:42.879575Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Add:0\", shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "a = tf.add(4, 6)\n",
    "\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ... Remember we first build the graph, then we execute in the session. \n",
    "### We simply just defined the tensor \"a\".\n",
    "\n",
    "\n",
    "\n",
    "### What kind of tensor is \"a\"? Scalar, Vector, or Matrix?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://www.chemometry.com/Images/Research/TAC/TAC1.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First we build our TensorFlow framework, then we execute them all at once. \n",
    "\n",
    "## In TensorFlow we initialize our \"Execution Phase\" by creating a session.\n",
    "\n",
    "\n",
    "## We donote our session by calling \"tf.Session()\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-08T15:41:44.676721Z",
     "start_time": "2017-11-08T15:41:44.662563Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "# How to get the value of a?\n",
    "# Create a session, assign it to variable sess so we can call it later\n",
    "# Within the session, evaluate the graph to fetch the value of a\n",
    "\n",
    "a = tf.add(4, 6)\n",
    "sess = tf.Session()\n",
    "\n",
    "print(sess.run(a))\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's say we now have a lot more variables in our framework...\n",
    "\n",
    "## Create a session to run and print operations a, b, and c."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-08T15:41:45.087965Z",
     "start_time": "2017-11-08T15:41:45.078081Z"
    }
   },
   "outputs": [],
   "source": [
    "a = tf.add(4, 6)\n",
    "b = tf.multiply(2, 10)\n",
    "c = tf.subtract(b, 5)\n",
    "\n",
    "# Create your Session called sess by calling tf.Session()\n",
    "# print a, b, c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How many of you guys have used the \"with\" block before? \n",
    "**Hint: You may have seen it earlier in the course opening files before pandas.read_csv()**\n",
    "\n",
    "Syntatically goes like this:\n",
    "\n",
    "**with** `expression` **as** `variable`:\n",
    "\n",
    "    variable (operation)\n",
    "    \n",
    "After the block is over the `expression` is terminated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-08T15:41:45.644644Z",
     "start_time": "2017-11-08T15:41:45.620770Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10, 20, 15]\n"
     ]
    }
   ],
   "source": [
    "# Now condense this a little bit\n",
    "\n",
    "a = tf.add(4, 6)\n",
    "b = tf.multiply(2, 10)\n",
    "c = tf.subtract(b, 5)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    added = sess.run([a, b, c])\n",
    "    print(added)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Session object encapsulates the environment in which Operation objects are executed, and Tensor objects are evaluated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What value will this print out?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-08T15:41:46.485587Z",
     "start_time": "2017-11-08T15:41:46.461001Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "x = 2\n",
    "y = 3\n",
    "\n",
    "op1 = tf.add(x, y)\n",
    "\n",
    "op2 = tf.multiply(x, y)\n",
    "\n",
    "op3 = tf.add(op2, op1)\n",
    "\n",
    "with tf.Session() as sess: \n",
    "    \n",
    "    op3 = sess.run(op3)\n",
    "\n",
    "print(op3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What happened\n",
    "\n",
    "![](assets/graph1.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why did I label one of these as `useless`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-08T15:41:47.561409Z",
     "start_time": "2017-11-08T15:41:47.532818Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "x = 2\n",
    "y = 3\n",
    "add_op = tf.add(x, y)\n",
    "\n",
    "mul_op = tf.multiply(x, y)\n",
    "\n",
    "useless = tf.multiply(x, add_op)\n",
    "\n",
    "add_op2 = tf.add(add_op, mul_op)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    z = sess.run(add_op2) \n",
    "    \n",
    "print(z)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is happening here?\n",
    "\n",
    "## Draw out what the Tensor Graph on your table with a partner.\n",
    "- How will this graph look different?\n",
    "\n",
    "![](assets/graph2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](assets/useless_nn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# You can also run two parts of the graph in the same session simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-08T15:41:49.680807Z",
     "start_time": "2017-11-08T15:41:49.646693Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15625\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "x = 2\n",
    "y = 3\n",
    "add_op = tf.add(x, y,)\n",
    "\n",
    "mul_op = tf.multiply(x, y)\n",
    "\n",
    "useless = tf.multiply(x, add_op)\n",
    "\n",
    "pow_op = tf.pow(add_op, mul_op)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    z, not_useless = sess.run([pow_op, useless])\n",
    "    \n",
    "print(z)\n",
    "\n",
    "print(not_useless)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How did I make my not useless variable not useless?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now let's add a few more peices to our graph. \n",
    "\n",
    "## First we will define the processor we would like to use for this Session. \n",
    "- **By doing so we are able to run multiple processes at once to cut down on compuation time.**\n",
    "\n",
    "![](assets/gpus.png)\n",
    "\n",
    "*Example: AlexNet Graph from the book “Hands-On Machine Learning with Scikit-Learn and TensorFlow”*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All we need to do is specify which placeholder variables we would like to be run by which CPU/GPU device. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-08T15:41:51.196865Z",
     "start_time": "2017-11-08T15:41:51.156705Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15625\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "# Creates a graph.\n",
    "\n",
    "x = 2\n",
    "y = 3\n",
    "with tf.device('/cpu:1'):\n",
    "    add_op = tf.add(x, y,)\n",
    "    mul_op = tf.multiply(x, y)\n",
    "    pow_op = tf.pow(add_op, mul_op)\n",
    "\n",
    "with tf.device('/cpu:2'):\n",
    "    useless = tf.multiply(x, add_op)\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    \n",
    "    # Creates a session with allow_soft_placement and log_device_placement set to True.\n",
    "    sess = tf.Session(config=tf.ConfigProto(\n",
    "          allow_soft_placement=True, log_device_placement=True))\n",
    "\n",
    "    z, not_useless = sess.run([pow_op, useless])\n",
    "    \n",
    "print(z)\n",
    "\n",
    "print(not_useless)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note:\n",
    "\n",
    "**\"allow_soft_placement=True\"** = If you would like TensorFlow to automatically choose an existing and supported device to run the operations in case the specified one doesn't exist, you can set allow_soft_placement to True in the configuration option when creating the session."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possible to break graphs into several\n",
    "chunks and run them parallelly\n",
    "across multiple CPUs, GPUs, or\n",
    "devices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is your Graph?\n",
    "\n",
    "- Consist of Two primary elements.\n",
    "\n",
    "### 1.  Graph Structure\n",
    " - All your nodes and edges in the graph\n",
    " - How all of your operations are composed together\n",
    "\n",
    "### 2. Graph Collections\n",
    " - All your global variables\n",
    " - All your training variables\n",
    " \n",
    "**Biggest thing worth noting, is making sure your graph does not get too big. If your graph is too large it will really slow down your computer. This is easy to do in jupyter notebooks because every variable in the kernel is assigned to the default graph.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to specify and call your graph\n",
    "\n",
    "### tf.Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-08T15:41:53.041757Z",
     "start_time": "2017-11-08T15:41:53.028975Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    a = 3\n",
    "    b = 5\n",
    "    x = tf.add(a, b)\n",
    "    \n",
    "sess = tf.Session(graph=g) # session is run on the graph g\n",
    "\n",
    "print(sess.run(x))\n",
    "# run session\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now lets start visualizing our results on TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is TensorBoard?\n",
    "### - TensorBoard is a visualization tool to inspect and understand our TensorFlow runs and graphs. \n",
    "### - Training a massive deep neural network can be confusing and complicated to understand.\n",
    "### - TensorBoard will make this a lot easier to understand, debug, and optimize our neural networks in TensorFlow.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](assets/tboard.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's build a graph together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-08T15:41:55.341736Z",
     "start_time": "2017-11-08T15:41:55.324217Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "a = 4\n",
    "\n",
    "b = 6\n",
    "\n",
    "x = tf.add(a, b)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    \n",
    "    # add this line to use TensorBoard.\n",
    "    writer = tf.summary.FileWriter('./graphs', sess.graph)\n",
    "    \n",
    "    print(sess.run(x))\n",
    "    \n",
    "writer.close() # close the writer when you’re done using it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now go to your terminal window.\n",
    "\n",
    "## Make sure you cd into `7.04-tensorflow`\n",
    "\n",
    "### Enter this code into the command line:  \n",
    "`tensorboard --logdir=\"./graphs\" --port 6006`\n",
    "\n",
    "\n",
    "### Then navigate to this url:\n",
    "`http://0.0.0.0:6006`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My graph was not labeled in TensorBoard. We always want labeled graphs in TensorBoard, it will make our lives a lot easier once the neural networks get bigger and we need to debug. \n",
    "\n",
    "### We can easily fix this by passing the name of every variable and operation in our graph. \n",
    "\n",
    "### We also need to convert our integers 2 and 3 to tensor objects so we can assign them a name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-08T15:41:56.859240Z",
     "start_time": "2017-11-08T15:41:56.836585Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "a = tf.constant(2, name='X')\n",
    "b = tf.constant(3, name='Y')\n",
    "add_op = tf.add(a, b, name='add')\n",
    "with tf.Session() as sess:\n",
    "    # add this line to use TensorBoard.\n",
    "    writer = tf.summary.FileWriter('./graphs', sess.graph)\n",
    "    print(sess.run(add_op))\n",
    "    \n",
    "writer.close() # close the writer when you’re done using it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refresh your TensorBoard webpage and take a look now at your labeled tensor graph. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Placeholders\n",
    "### Remember Tensor Flow has 2 Phases \n",
    "\n",
    "#### 1. Assemble a graph\n",
    "#### 2. Execute graph with Session\n",
    "\n",
    "**This allows us to assemble the graph first without knowing the values needed for\n",
    "computation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analogy:\n",
    "## Can define the function f(x, y) = x * 2 + y without knowing the values of x or y. \n",
    "- x, y are **placeholders** for the actual values.\n",
    "\n",
    "## Why do we do this?\n",
    "- We, or our clients, can later supply their own data when they need to execute the computation. \n",
    "\n",
    "**We will store our values in a dictionary until execution.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With someone next to you or by yourself explain what this cell is doing.\n",
    "# What is this cell going to output? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-08T15:41:58.946015Z",
     "start_time": "2017-11-08T15:41:58.930553Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 6.  7.  8.]\n"
     ]
    }
   ],
   "source": [
    "# create a placeholder of type float 32-bit, shape is a vector of 3 elements\n",
    "a = tf.placeholder(tf.float32, shape=[3])\n",
    "\n",
    "# create a constant of type float 32-bit, shape is a vector of 3 elements\n",
    "b = tf.constant([5, 5, 5], tf.float32)\n",
    "\n",
    "# use the placeholder as you would a constant or a variable\n",
    "c = a + b # Short for tf.add(a, b)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "# feed [1, 2, 3] to placeholder a via the dict {a: [1, 2, 3]}\n",
    "\n",
    "# Remember: (tf.Session.run(fetch, feed_dict)\n",
    "    print(sess.run(c, {a: [1, 2, 3]})) # the tensor a is the key, not the string ‘a’"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Placeholders Shape is Important\n",
    "\n",
    "### Code:\n",
    "**var = tf.placeholder(tf.float32, shape=[])**\n",
    "\n",
    "### Caveats:\n",
    "- ** if shape=None** means that tensor of any shape will be accepted as value for placeholder.\n",
    "- **if shape=None** is easy to construct graphs, but nightmarish for debugging\n",
    "\n",
    "So... its best to include the shape. This will become way more clear once we start building neural networks.\n",
    "\n",
    "Lastly, you can feed_dict any feedable tensor. Placeholder is just a way to indicate that something must be fed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-06T16:24:58.094244Z",
     "start_time": "2017-11-06T16:24:58.006170Z"
    }
   },
   "source": [
    "# Variables \n",
    "\n",
    "## How is a variable different than a place holder?\n",
    "\n",
    "## In the following equation what values would be \"placeholders\" and what values would be \"variables\"?\n",
    "\n",
    "## y = w * x + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-08T15:42:00.691593Z",
     "start_time": "2017-11-08T15:42:00.685811Z"
    }
   },
   "outputs": [],
   "source": [
    "# create placeholder tensors\n",
    "\n",
    "x = tf.placeholder(tf.float32, name='x')\n",
    "y = tf.placeholder(tf.float32, name='y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-08T15:42:01.586129Z",
     "start_time": "2017-11-08T15:42:01.574424Z"
    }
   },
   "outputs": [],
   "source": [
    "# create Variable tensors\n",
    "\n",
    "w = tf.Variable(0.0, name='w')\n",
    "b = tf.Variable(0.0, name='b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we have added the placeholders and variables for a simple linear function. Next we want to run our session.\n",
    "\n",
    "\n",
    "# BUT Wait...\n",
    "\n",
    "# Before I run my sesssion, I need to make sure of one more thing. \n",
    "\n",
    "# All variables in TensorFlow must be initialized.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The easiest way to initialize all of your variables, is to do it all at once right before you run your session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-08T15:42:03.813174Z",
     "start_time": "2017-11-08T15:42:03.772540Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 3 4 5 6]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "a_list = [1, 2, 3, 4, 5]\n",
    "\n",
    "x = tf.placeholder(tf.int32, shape=[5], name='x')\n",
    "\n",
    "m = tf.Variable(1, name='m')\n",
    "\n",
    "b = tf.Variable(1, name='b')\n",
    "\n",
    "linear_func = tf.add(tf.multiply(x, m), b)\n",
    "\n",
    "# Remember everything is run in the session, we are just define what we want to do here.\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(init)\n",
    "    \n",
    "    \n",
    "    line = sess.run(linear_func, feed_dict={x: a_list})\n",
    "\n",
    "\n",
    "    print(line)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There are a few other ways to initialize Variables, for more information I suggest you check the TensorFlow documentation. `tf.global_variables_initializer()` is by far the most common way to initialize your variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now on your own or with a partner build the following:\n",
    "1. One tensorflow placeholder\n",
    "2. One tensorflow Variable\n",
    "3. Build your feed_dict with 5 values\n",
    "4. Add/Multiply (Or any operation) in your session and print out your results.\n",
    "\n",
    "**Much like the example above**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-08T15:42:05.320854Z",
     "start_time": "2017-11-08T15:42:05.318273Z"
    }
   },
   "outputs": [],
   "source": [
    "# Build and run your tensor flow model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Now we know most of the pieces, it is time to bring them all together. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before we jump right into our first Neural Network lets, run through first building a linear model with TensorFlow. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1: Build our model graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1: Import our packages and data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-08T15:42:08.266369Z",
     "start_time": "2017-11-08T15:42:07.965668Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "data_file = 'assets/fire_theft.csv'\n",
    "\n",
    "# Phase 1: Assemble the graph\n",
    "# Step 1: read in data from the .csv file\n",
    "data = pd.read_csv(data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2: Define and name our place holders**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-08T15:45:22.632446Z",
     "start_time": "2017-11-08T15:45:22.626374Z"
    }
   },
   "outputs": [],
   "source": [
    "# Step 2: create placeholders for input X (number of fire) and label Y (number of theft)\n",
    "# Both have the type float32\n",
    "\n",
    "X = tf.placeholder(tf.float32, name='X')\n",
    "\n",
    "# WHAT IS OUR SECOND PLACE HOLDER?\n",
    "\n",
    "Y = tf.placeholder(tf.float32, name='Y_actuals')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3: Define our Variables**\n",
    "- What are our Variables?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-08T15:45:23.412954Z",
     "start_time": "2017-11-08T15:45:23.400446Z"
    }
   },
   "outputs": [],
   "source": [
    "# Step 3: create weight and bias, initialized to 0\n",
    "# name your variables w and b\n",
    "\n",
    "w = tf.Variable(0.0, name='weight')\n",
    "b = tf.Variable(0.0, name='bias')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Okay, before we move forward what are we trying to do with this linear regression?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4: Build our Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-08T15:45:45.500295Z",
     "start_time": "2017-11-08T15:45:45.494888Z"
    }
   },
   "outputs": [],
   "source": [
    "# Simple Linear Model\n",
    "\n",
    "Y_predictions = X* w + b\n",
    "\n",
    "# Y = X * w + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is our loss?\n",
    "**Hint: We will use mean squared error.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 5: Indicate our loss**\n",
    "- Use TensorFlows function to take the square\n",
    "- `tf.reduce_mean` gives us the mean of our squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-08T15:45:46.491343Z",
     "start_time": "2017-11-08T15:45:46.479510Z"
    }
   },
   "outputs": [],
   "source": [
    "# Step 5: use the square error as the loss function\n",
    "# Make all values positive\n",
    "# find mean loss\n",
    "\n",
    "squares = tf.square(Y - Y_predictions)\n",
    "\n",
    "loss = tf.reduce_mean(squares, name='loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How do we reduce our loss?\n",
    "- What mathmatical function helps us reduce loss?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-07T21:09:23.614332Z",
     "start_time": "2017-11-07T21:09:23.526730Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 6: Build loss optimizer**\n",
    "- We will use gradient descent right now but there are several different flavors of gradient descent that are sometimes better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**More Optimizers**\n",
    "\n",
    "tf.train.GradientDescentOptimizer\n",
    "\n",
    "tf.train.AdagradOptimizer\n",
    "\n",
    "tf.train.MomentumOptimizer\n",
    "\n",
    "tf.train.AdamOptimizer\n",
    "\n",
    "tf.train.ProximalGradientDescentOptimizer\n",
    "\n",
    "tf.train.ProximalAdagradOptimizer\n",
    "\n",
    "tf.train.RMSPropOptimizer\n",
    "\n",
    "And more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-08T15:45:48.652234Z",
     "start_time": "2017-11-08T15:45:48.567776Z"
    }
   },
   "outputs": [],
   "source": [
    "# Step 6: using gradient descent with learning rate of 0.001 to minimize loss\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.001).minimize(loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 7: Specify our variable initializer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-08T15:45:49.315392Z",
     "start_time": "2017-11-08T15:45:49.311634Z"
    }
   },
   "outputs": [],
   "source": [
    "# Step 7: initialize the necessary variables, in this case, w and b\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2: Execute our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 8: Initialize our session using a with block**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-08T15:45:53.139297Z",
     "start_time": "2017-11-08T15:45:50.544312Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 2069.6319333978354\n",
      "Epoch 1: 2117.0123581953535\n",
      "Epoch 2: 2092.302723001866\n",
      "Epoch 3: 2068.5080461938464\n",
      "Epoch 4: 2045.591184088162\n",
      "Epoch 5: 2023.5146448101316\n",
      "Epoch 6: 2002.2447619835536\n",
      "Epoch 7: 1981.748338803649\n",
      "Epoch 8: 1961.9944411260742\n",
      "Epoch 9: 1942.9520116143283\n",
      "Epoch 10: 1924.5930823644712\n",
      "Epoch 11: 1906.8898800636332\n",
      "Epoch 12: 1889.8164505837929\n",
      "Epoch 13: 1873.347133841543\n",
      "Epoch 14: 1857.4588400604468\n",
      "Epoch 15: 1842.1278742424079\n",
      "Epoch 16: 1827.332495119955\n",
      "Epoch 17: 1813.0520579712022\n",
      "Epoch 18: 1799.2660847636982\n",
      "Epoch 19: 1785.9562132299961\n",
      "Epoch 20: 1773.1024853109072\n",
      "Epoch 21: 1760.689129482884\n",
      "Epoch 22: 1748.6984157081515\n",
      "Epoch 23: 1737.1138680398553\n",
      "Epoch 24: 1725.920873066732\n",
      "Epoch 25: 1715.1046249579008\n",
      "Epoch 26: 1704.6500954309377\n",
      "Epoch 27: 1694.5447134910141\n",
      "Epoch 28: 1684.7746311347667\n",
      "Epoch 29: 1675.328450968245\n",
      "Epoch 30: 1666.1935385839038\n",
      "Epoch 31: 1657.3584002084322\n",
      "Epoch 32: 1648.8122658529207\n",
      "Epoch 33: 1640.5440742547091\n",
      "Epoch 34: 1632.5446836102221\n",
      "Epoch 35: 1624.8043315147183\n",
      "Epoch 36: 1617.3126799958602\n",
      "Epoch 37: 1610.0622532456405\n",
      "Epoch 38: 1603.0433557207386\n",
      "Epoch 39: 1596.2479176106197\n",
      "Epoch 40: 1589.668056331575\n",
      "Epoch 41: 1583.2965242617897\n",
      "Epoch 42: 1577.126371285745\n",
      "Epoch 43: 1571.1501190634\n",
      "Epoch 44: 1565.360979151513\n",
      "Epoch 45: 1559.7523780798629\n",
      "Epoch 46: 1554.3184364555138\n",
      "Epoch 47: 1549.0529469620615\n",
      "Epoch 48: 1543.950059985476\n",
      "Epoch 49: 1539.0050282141283\n",
      "Epoch 50: 1534.211797797609\n",
      "Epoch 51: 1529.56534988646\n",
      "Epoch 52: 1525.0607591186251\n",
      "Epoch 53: 1520.6934648507852\n",
      "Epoch 54: 1516.4585935090713\n",
      "Epoch 55: 1512.3524023861364\n",
      "Epoch 56: 1508.3695780125756\n",
      "Epoch 57: 1504.5066588066873\n",
      "Epoch 58: 1500.7606269073274\n",
      "Epoch 59: 1497.126336559476\n",
      "Epoch 60: 1493.600210891061\n",
      "Epoch 61: 1490.1794991287668\n",
      "Epoch 62: 1486.8605145300749\n",
      "Epoch 63: 1483.639419928193\n",
      "Epoch 64: 1480.5144186365596\n",
      "Epoch 65: 1477.4811065652452\n",
      "Epoch 66: 1474.5376660533782\n",
      "Epoch 67: 1471.6799176652871\n",
      "Epoch 68: 1468.9063155567717\n",
      "Epoch 69: 1466.2136880280007\n",
      "Epoch 70: 1463.5996563179153\n",
      "Epoch 71: 1461.0614086978492\n",
      "Epoch 72: 1458.597208841216\n",
      "Epoch 73: 1456.2043069711044\n",
      "Epoch 74: 1453.8807724802089\n",
      "Epoch 75: 1451.6242183893032\n",
      "Epoch 76: 1449.432753210976\n",
      "Epoch 77: 1447.3042320180018\n",
      "Epoch 78: 1445.237068621615\n",
      "Epoch 79: 1443.228872676177\n",
      "Epoch 80: 1441.2782130186733\n",
      "Epoch 81: 1439.3831422174615\n",
      "Epoch 82: 1437.542224922173\n",
      "Epoch 83: 1435.7540219968096\n",
      "Epoch 84: 1434.0160684508405\n",
      "Epoch 85: 1432.3276573866606\n",
      "Epoch 86: 1430.687153330871\n",
      "Epoch 87: 1429.093016880254\n",
      "Epoch 88: 1427.543719962062\n",
      "Epoch 89: 1426.038033108981\n",
      "Epoch 90: 1424.5748210840281\n",
      "Epoch 91: 1423.1531702368743\n",
      "Epoch 92: 1421.771026852585\n",
      "Epoch 93: 1420.4274983895677\n",
      "Epoch 94: 1419.121967994741\n",
      "Epoch 95: 1417.85251878131\n",
      "Epoch 96: 1416.618930517208\n",
      "Epoch 97: 1415.4196022436731\n",
      "Epoch 98: 1414.2534379121803\n",
      "Epoch 99: 1413.1202843011845\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    \n",
    "    # Now that our session is initialized we need to run our initializer to initialize weights and biases\n",
    "    sess.run(init)\n",
    "    \n",
    "    \n",
    "    # STEP 9: Define our TensorBoard writer\n",
    "    # we need to define our file location, we want it to be different than our last file location\n",
    "    # we will also want to define the graph we would like to pass. This is the default location\n",
    "    writer = tf.summary.FileWriter('./graphs/lin_reg', sess.graph)\n",
    "    \n",
    "    \n",
    "    # STEP 10: Train our model using batches and a feed_dict\n",
    "    for i in range(100):  # run 100 epochs\n",
    "        total_loss = 0    # set our total_loss counter\n",
    "        for x, y in data.values: # define our x and y data from our data set we will assign these to our feed_dict\n",
    "            \n",
    "            # Session runs optimizer to minimize loss and fetch the value of loss. Name the received value as l\n",
    "            _, l = sess.run([optimizer, loss], feed_dict={X: x, Y: y})\n",
    "            \n",
    "            # Why do I have a \"_\" value?\n",
    "            \n",
    "            # Increase our total_loss by \"1\" (*This is not a one*)\n",
    "            total_loss += l\n",
    "        \n",
    "        # Print out our total_loss at each Epoch\n",
    "        print(\"Epoch {0}: {1}\".format(i, total_loss / len(data)))\n",
    "        \n",
    "    # close the writer when you're done using it\n",
    "    writer.close()\n",
    "    \n",
    "    # Lastly we want to run our values for weight and bias, so we can plot them afterwards.\n",
    "    w, b = sess.run([w, b])\n",
    "        \n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-08T15:46:00.654314Z",
     "start_time": "2017-11-08T15:46:00.465364Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJzt3Xt0VfWZ//H3I6AI2ioBrT8uCTNS\ncUQMISp4pRUtHRV1Bpa1qTIzjnS8VMdOVZRx6XTqjC6dap3lpXSq0pIRa5Wf2vGCFxj9YcUGxYpc\nBCtIQCGgUGgUCHl+f+wTzklybsm57bPzea11VnL2/ubshx3ynO95vvv73ebuiIhIdO1X6gBERKSw\nlOhFRCJOiV5EJOKU6EVEIk6JXkQk4pToRUQiToleRCTilOhFRCJOiV5EJOJ6lzoAgIEDB3pVVVWp\nwxARKStLlizZ4u6DMrULRaKvqqqioaGh1GGIiJQVM1uXTTuVbkREIk6JXkQk4pToRUQiLhQ1+mT2\n7NlDY2MjX3zxRalDkSz07duXIUOG0KdPn1KHIiIdhDbRNzY2cvDBB1NVVYWZlTocScPd2bp1K42N\njQwfPrzU4YhIB6Et3XzxxRdUVFQoyZcBM6OiokKfvkS6oL4eqqpgv/2Cr/X1hTtWaHv0gJJ8GdHv\nSiR79fUwfTo0NwfP160LngPU1eX/eKHt0YuIRNXMmfEk36a5OdheCEr0afTq1Yvq6mpGjRrFueee\ny7Zt27r9WlVVVWzZsiVtm0ceeYSrrroqbZuFCxfy+uuvdzsOESm9jz7q2vZcRSbRF6LedeCBB7J0\n6VKWLVvGgAEDuO+++3J/0Rwp0YuUv2HDurY9V5FI9G31rnXrwD1e78rn4Mb48ePZsGHDvud33nkn\nxx9/PKNHj+aWW27Zt/38889n7NixHHPMMcyaNSvj6z788MN89atf5fTTT2fRokX7tj/zzDOceOKJ\njBkzhokTJ7Jp0ybWrl3Lgw8+yN133011dTWvvfZa0nYiEm633Qb9+rXf1q9fsL0g3L3kj7Fjx3pH\ny5cv77QtlcpK9yDFt39UVmb9Ekn179/f3d1bWlp8ypQp/txzz7m7+wsvvOCXXXaZt7a2+t69e/3s\ns8/2//3f/3V3961bt7q7e3Nzsx9zzDG+ZcuWWIyV3tTU1O71N27c6EOHDvXNmzf7rl27/KSTTvIr\nr7zS3d0//fRTb21tdXf3n/3sZ/7973/f3d1vueUWv/POO/e9Rqp2pdCV35lITzdnTpCjzIKvc+Z0\n/TWABs8ix4b6qptsFare9fnnn1NdXc3atWsZO3YsZ555JgDz589n/vz5jBkzBoCdO3eyevVqTjvt\nNO69917mzZsHwPr161m9ejUVFRVJX3/x4sVMmDCBQYOCxecuvPBC3n//fSCYR3DhhRfy8ccfs3v3\n7pTXp2fbTkTCpa6uMFfYJBOJ0k2h6l1tNfp169axe/fufTV6d+fGG29k6dKlLF26lDVr1nDppZey\ncOFCXnrpJX7729/yzjvvMGbMmIzXlqe6LPF73/seV111Fe+++y4//elPU75Otu1EpOfKmOjN7CEz\n22xmy5Ls+4GZuZkNjD03M7vXzNaY2e/NrKYQQXdU6HrXl7/8Ze69917uuusu9uzZwze+8Q0eeugh\ndu7cCcCGDRvYvHkz27dv59BDD6Vfv36sXLmSN954I+3rnnjiiSxcuJCtW7eyZ88eHn/88X37tm/f\nzuDBgwGYPXv2vu0HH3wwO3bsyNhORKRNNj36R4BJHTea2VDgTCCxQPJNYETsMR14IPcQM6urg1mz\noLISzIKvs2bl92PRmDFjOO6445g7dy5nnXUW3/72txk/fjzHHnssU6ZMYceOHUyaNImWlhZGjx7N\nzTffzLhx49K+5hFHHMGtt97K+PHjmThxIjU18ffFW2+9lalTp3LqqacycODAfdvPPfdc5s2bt28w\nNlU7EZE2FtTzMzQyqwJ+4+6jErb9GvhX4Cmg1t23mNlPgYXu/miszSpggrt/nO71a2trveONR1as\nWMHRRx/dtX+NlJR+ZyLFZWZL3L02U7tu1ejNbDKwwd3f6bBrMLA+4XljbJuIiJRIl6+6MbN+wEzg\nrGS7k2xL+pHBzKYTlHcYVqhZAiIi0q0e/Z8Dw4F3zGwtMAR4y8y+QtCDH5rQdgiwMdmLuPssd691\n99q2ywtFRCT/upzo3f1ddz/M3avcvYogude4+yfA08AlsatvxgHbM9XnRUSksLK5vPJR4LfAUWbW\naGaXpmn+LPAHYA3wM+CKvEQpIiLdlrFG7+4XZdhflfC9A1fmHpaIiORLJGbGFkriMsVTp06lueMC\n0l2wcOFCzjnnHACefvppbr/99pRtt23bxv3339/lY9x6663cddddGdsddNBBafd39/giEk5K9Gkk\nLlO8//778+CDD7bb7+60trZ2+XUnT57MjBkzUu4vdaIt9fFFJL+U6LN06qmnsmbNGtauXcvRRx/N\nFVdcQU1NDevXr2f+/PmMHz+empoapk6dum9phOeff56RI0dyyimn8OSTT+57rcQbjGzatIkLLriA\n4447juOOO47XX3+dGTNm8MEHH1BdXc11110HpF4W+bbbbuOoo45i4sSJrFq1KmnsH374IePHj+f4\n44/n5ptv3rd9586dnHHGGdTU1HDsscfy1FNPAXQ6fqp2IlIeymP1yn/8R1i6NL+vWV0N99yTVdOW\nlhaee+45Jk0KVoJYtWoVDz/8MPfffz9btmzhRz/6ES+99BL9+/fnjjvu4Mc//jHXX389l112Ga+8\n8gpHHnkkF154YdLXvvrqqzn99NOZN28ee/fuZefOndx+++0sW7aMpbF/8/z581m9ejVvvvkm7s7k\nyZN59dVX6d+/P3PnzuXtt9+mpaWFmpoaxo4d2+kY11xzDZdffjmXXHJJu5un9O3bl3nz5vGlL32J\nLVu2MG7cOCZPntzp+C0tLUnb6T6xIuWhPBJ9ibQtUwxBj/7SSy9l48aNVFZW7lvH5o033mD58uWc\nfPLJAOzevZvx48ezcuVKhg8fzogRIwD4zne+k/RGJK+88gq/+MUvgGBM4Mtf/jKfffZZuzaplkXe\nsWMHF1xwAf1iK7pNnjw56b9j0aJFPPHEEwBcfPHF3HDDDUBQerrpppt49dVX2W+//diwYUPSG5ek\naveVr3ylC2dTREqlPBJ9lj3vfGur0XfUv3//fd+7O2eeeSaPPvpouzZLly7NW4+3bVnk7373u+22\n33PPPVkfI1m7+vp6mpqaWLJkCX369KGqqirpMsfZthORcFKNPkfjxo1j0aJFrFmzBoDm5mbef/99\nRo4cyYcffsgHH3wA0OmNoM0ZZ5zBAw8Ei3zu3buXP/7xj52WIk61LPJpp53GvHnz+Pzzz9mxYwfP\nPPNM0mOcfPLJzJ07FwiSdpvt27dz2GGH0adPHxYsWMC6deuA5EshJ2snIuVBiT5HgwYN4pFHHuGi\niy5i9OjRjBs3jpUrV9K3b19mzZrF2WefzSmnnEJlZWXSn//JT37CggULOPbYYxk7dizvvfceFRUV\nnHzyyYwaNYrrrrsu5bLINTU1XHjhhVRXV/PXf/3XnHrqqSmPcd9993H88cezffv2fdvr6upoaGig\ntraW+vp6Ro4cCdDp+KnaiUh5yGqZ4kLTMsXRoN+ZSHEVdJliEREpH0r0IiIRF+pEH4aykmRHvyuR\n8Aptou/bty9bt25VAikD7s7WrVvp27dvqUMRkSRCex39kCFDaGxspKmpqdShSBb69u3LkCFDSh2G\niCQR2kTfp08fhg8fXuowRETKXmhLNyIikh9K9CIiEadELyIScUr0IiIRl83NwR8ys81mtixh251m\nttLMfm9m88zskIR9N5rZGjNbZWbfKFTgIiKSnWx69I8AkzpsexEY5e6jgfeBGwHM7C+AbwHHxH7m\nfjPrlbdoRUSkyzImend/Ffi0w7b57t4Se/oG0HYB9XnAXHff5e4fAmuAE/IYr4iIdFE+avR/BzwX\n+34wsD5hX2Nsm4iIlEhOid7MZgItQNvdLJLd7ijpGgZmNt3MGsysQbNfRUQKp9uJ3symAecAdR5f\nkKYRGJrQbAiwMdnPu/ssd69199pBgwZ1NwwREcmgW4nezCYBNwCT3b05YdfTwLfM7AAzGw6MAN7M\nPUwREemujGvdmNmjwARgoJk1ArcQXGVzAPBi7KbTb7j7P7j7e2b2K2A5QUnnSnffW6jgRUQks9De\nSlBERNLTrQRFRARQohcRiTwlehGRiFOiFxGJOCV6EZGIU6IXEYk4JXoRkYhTohcRiTglehGRiFOi\nFxGJOCV6EZGIU6IXEYk4JXoRkYhTohcRiTglehGRiFOiFxGJOCV6EZGIU6IXEYk4JXoRkYjLmOjN\n7CEz22xmyxK2DTCzF81sdezrobHtZmb3mtkaM/u9mdUUMngREcksmx79I8CkDttmAC+7+wjg5dhz\ngG8CI2KP6cAD+QlTRES6K2Oid/dXgU87bD4PmB37fjZwfsL2X3jgDeAQMzsiX8GKiEjXdbdGf7i7\nfwwQ+3pYbPtgYH1Cu8bYtk7MbLqZNZhZQ1NTUzfDEBGRTPI9GGtJtnmyhu4+y91r3b120KBBeQ5D\nRETadDfRb2orycS+bo5tbwSGJrQbAmzsfngiIpKr7ib6p4Fpse+nAU8lbL8kdvXNOGB7W4lHRERK\no3emBmb2KDABGGhmjcAtwO3Ar8zsUuAjYGqs+bPAXwJrgGbgbwsQs4iIdEHGRO/uF6XYdUaStg5c\nmWtQIiKSP5oZKyIScUr0IiIRp0QvIhJxSvQiIhGnRC8iEnFK9CIiEadELyIScUr0IiIRp0QvIhJx\nSvQiIhGnRC8iEnFK9CIiEadELyIScUr0IiIRp0QvIhJxSvQiIqXw9ttw3XWwaVPBD6VELyJSDC0t\ncN99YBY8amrgrrvgpZcKfmglehGRQmlshIsuChJ7nz5w1VXxfQcdBLNnw7e/XfAwckr0Znatmb1n\nZsvM7FEz62tmw81ssZmtNrPHzGz/fAUrIhJ6v/kNDB0aJPehQ2Hu3Pi+SZNgxQpwhx074JJLgnYF\n1u1Eb2aDgauBWncfBfQCvgXcAdzt7iOAz4BL8xGoiEgo7dwJM2bESzLnnhv05Nv86Efw+edBcn/u\nORg5sugh5lq66Q0caGa9gX7Ax8DXgV/H9s8Gzs/xGCIi4bJ0KZxySpDYDz4Y7rgjvu/oo2HBgiCx\nu8PMmdC3b+liJYdE7+4bgLuAjwgS/HZgCbDN3VtizRqBwbkGKSJSUnv3wgMPxHvtY8bAokXx/Zdd\nBps3B4l9+XKYMKFkoSaTS+nmUOA8YDjwf4D+wDeTNPUUPz/dzBrMrKGpqam7YYiIFMaGDVBXFyT2\n3r3hiivi+/r1CwZS9+4NkvusWTBoUOlizSCX0s1E4EN3b3L3PcCTwEnAIbFSDsAQYGOyH3b3We5e\n6+61g0J8gkSkB3n2WaisDJL7kCHw3/8d33fWWUFv3R3+9KdgIHW/8rhwMZcoPwLGmVk/MzPgDGA5\nsACYEmszDXgqtxBFRApk50646aZ4Sebss+Gjj+L7f/hDaG4OkvsLLwT19zLUO3OT5Nx9sZn9GngL\naAHeBmYB/wPMNbMfxbb9PB+BiojkxTvvwPe+B6+91nnfUUfB/ffD179e/LgKKKfPHe5+i7uPdPdR\n7n6xu+9y9z+4+wnufqS7T3X3XfkKVrqmvh6qqoJPl1VVwXORHmfvXnjwwXivvbq6fZL/+78PliFw\nh5UrI5fkIYcevYRbfT1Mnx586gRYty54DsH4kkikbdwI11+fvHfTr1/Qa7/44rKpseeqZ/wre6CZ\nM+NJvk1zc7BdJJIeeyz46GoGgwe3T/IdB1KnTesxSR7Uo4+sxPGkbLaLlJ3Nm+Hww1Pv/+EP4Qc/\ngAMPLF5MIdVz3tJ6mGHDurZdpCzMmROvtSdL8i++GJ+RevPNSvIxSvQRddttQSkyUb9+wXaRsrFn\nD4wdG0/uF1/cuc2yZfHkPnFi8WMsA0r0EVVXF0zWa5v7UVkZPNdArITeW2/FE/v++wfPE33ta8Ha\n7m3J/ZhjShNnGVGij7C6Oli7Flpbg69K8hJa114bT+5jx3be/8QT8cT+yivQq1fxYyxjGowVkeJr\naoLDDku9v3fv4Nr2AQOKF1OEqUcvIsXx6KPxXnuyJD9zZrzXvmdPl5K8Jgempx69iBTGnj1w0knQ\n0JC6zbvvwqhROR1GkwMzU49eRPLn7bfbD6R2TPKnn95+IDXHJA+aHJgNJXoRyc0//VM8udfUdN7/\n+OPxxL5wYd4HUjU5MDOVbkSkazINpJrBli1FG0gdNiwo1yTbLgH16EUks5kz0w+kzpgR77W3thb1\nahlNDsxMPXoR6eyLLzIvH/DOOzB6dHHiSaNtwHXmzKBcM2xYkOQ1EBunRC8igYcfhr/7u/Rt9uwJ\nrnEPmbo6JfZ0VLoR6cnayjFmyZP8v/xLvCTjHsokL5nptybSk6xcmfm+p++/DyNGFCceKQolepGo\nO/98eOqp9G3cixOLlEROpRszO8TMfm1mK81shZmNN7MBZvaima2OfT00X8GKSBa++KJ9SSZZkp8z\np31JRiIt1xr9T4Dn3X0kcBywApgBvOzuI4CXY89FpJBmz44n9lRXy3zxRTyxa+SyR+l2ojezLwGn\nAT8HcPfd7r4NOA+YHWs2Gzg/1yBFJInEXvvf/E3n/VOmtO+1H3BA0UOUcMilR/9nQBPwsJm9bWb/\nZWb9gcPd/WOA2Nc0U+hEJGurVrVP7qnatCX2xx8vbnwSWrkk+t5ADfCAu48B/kQXyjRmNt3MGsys\noampKYcwRCJsypR4Yh85MnmbxF77V79a3PikLOSS6BuBRndfHHv+a4LEv8nMjgCIfd2c7IfdfZa7\n17p77aBBg3IIQyRCOg6kPvFE5zazZ2sgVbqk24ne3T8B1pvZUbFNZwDLgaeBabFt04AM13WJ9HC/\n/GXmgdTPP48n9ksuKW58UvZyvY7+e0C9me0P/AH4W4I3j1+Z2aXAR8DUHI8hEj2pauxtLrgAnnyy\nOLFI5OWU6N19KVCbZNcZubyuSOS88w5UV6dvs2JF6jq8SA40M1akUEaNgvfeS99GNXYpAi1qFjK6\nyXEZ6ziQmizJ3367BlKl6NSjDxHd5LgM3XMPXHtt+jZ/+lPnO2OIFJF5CHoVtbW13pDuTvE9RFVV\n8luiVVbC2rXFjkZSyjSQeuSRsHp1cWKRHs3Mlrh7snHSdlS6CRHd5Dik3nwz84zUJUvi5RgleQkZ\nlW5CRDc5DpEDDoDdu9O3CcGnYZFsqEcfIrrJcQk1N7fvtSdL8ldfrYFUKUtK9CFSVwezZgU1ebPg\n66xZGogtmJkz44m9f//kbf74x3hi/8lPihufSJ6odBMyuslxgWUaSAX11iVy1KOXaGtoyDyQunCh\nSjISaerRS/QcdFBw7Xo6SujSg6hH30MlzsAdODB4lO1s3I4zUpMl+SuuUK9deiz16HugjjNwt26N\n7yub2bj/+Z/BVTDpbN8OX/pSceIRCTHNjO2BUs3ATRTK2biZBlKHDIH164sTi0gIaGZsBOVrwbNs\nZtqGYjbu8uWZB1IXL46XY5TkRZJSoi8TbeWWdeuCnNZWYkmW7DO9IWQz07Zks3FPOCGe2I85Jnmb\nxFr7CScUNz6RMqREXyZmzozX1Ns0Nwfb29TXB4Oq3/lO+jeEZDNwExV1Nu6uXe177b/7Xec2t96q\ngVSRHCjRl4lMC5619fgTB1bbdHxD6DgDt6IieBRtNu7998cTe9++ydskzki95ZYCBiMSfTknejPr\nZWZvm9lvYs+Hm9liM1ttZo/F7icrOUpVSmnbnqzHn6jjG0VdXTDY2toKW7YEj9bWYFtBknxir/3K\nKzvvP/zw9r32gw8uQBAiqUX5pj/56NFfA6xIeH4HcLe7jwA+Ay7NwzF6vEwLnmUaPC16zX3FiswD\nqa+/Hk/sn3wS6T80CbeujIGVJXfv9gMYArwMfB34DWDAFqB3bP944IVMrzN27FiXzObMca+sdDcL\nvs6ZE99XWZnYHW7/6NevfduCGTcudRBtjxTmzAniLEnc0uOl+vuprCx1ZOkBDZ5Frs61R38PcD3Q\nGnteAWxz95bY80ZgcI7H6LE69nAhXm7pWGJJNcBaUVHAmvvu3e177W+80bnNP/9zVgOp2Qw2ixRK\n1G/60+1Eb2bnAJvdfUni5iRNk/51m9l0M2sws4ampqbuhhFZXf0omWyJ4zlzgtp7XpP8gw/GE/sB\nByRvs317PLH/679m9bJR/0OTcMs0Blb2sun2J3sA/07QY18LfAI0A/WodJNWuvJLolB9lMxUjhk4\nMOdDhOrfKz1OuZYOKXTpxt1vdPch7l4FfAt4xd3rgAXAlFizacBT3T1G1HSll17SHu6qVZkHUhct\niv9N5OETme6uJaUU9Zv+FOI6+huA75vZGoKa/c8LcIyy1JU6dHc+SuZ01copp8QT+8iRydskdnhO\nOqkLL55Z1P/QJPwSLzku2GXGJaJFzYpov/2Sj0eaBf+5EnVcYRKCHm6q5NfV9uzalXqyUpsbb4R/\n+7f0bUSkZLSoWQh1pZfe1R5uqk8L06YFS7FXVcFN9u+ZZ6Ru2xbvtSvJC9GeSNRTqEdfRF3udXdB\nqk8LnvRCqI6NSv9/QMKpkP9nJXfq0YdQIevQbZ8Kavkdju17JPNNnqWqUouESWaa3xANSvRpFOIj\na0EGfMxYuy5I7L8j+bK9lpD+n+ebZXl9eqbfh0oM+af5DdGgRJ9CqNe+6Li0bxKv8LV2yb2jcpsI\nkun3EerfVxmL/ESiniKbi+0L/QjjhKl8TuDJdpJUWtddl3ni0qZNSSd+lGztmzzK9Pso5oSrvPw+\ny0S5TiTqKchywlTJk7yHNNGbJU8cZl17nZz+UDIl9hSLhHVMRJdfXrrElK+kmOn3ka/fVyY9MfH1\npDe2cqNEn6Nse4iZ/ggqKrJ7HXd3X7Ikc2J/7LFC/HMLIp9JMSw9+u4eR8lSCkGJPkfZJKlMbebM\nSZ2v9/U0u9lrLwf5Ln9lOtfF6Gl355NDT/wUIMWhRJ8HmXph3ell9mZ35sR+6qlF/XcWSr7LKZl+\nH8XoNXfnzUsLtkmhKNHnWbIkkm3d+PvclTm5f/JJKf95BRHFBNed3nmxxg/yRWWm8qFEn0ep/rjT\n1t8jXJLJVlRLFl1NhOX0hhfV31lUKdHnUao/1IqK+B/Fn7EmY2I/m2d63B+NeofllTzL6U1Jsk/0\nmjCVhVSzAP9z60X8qTmYkvQBRyZtU//LVqoqnf3MWVZ5jtYI6YHKaQlmzYSNJi1qloWqqmCmZS9a\naKFP+sbXXgs//nFR4go7LYhVftr+r3dUWRks2SHhokXN8uXBB/etI5MyyX/2WfxTrpL8PloQq/zo\nTl/RpESfTOI6Mpdf3mn3eoYwsMKpnxNL7occUoIgw09lgPJTTmUmyV5kEn1OKxf+4Q+ZFwm77jn6\n9wsWCBvGerZu1aJZmZRiQSytYJm7KN9Sr8fKZsS20I9cr7rp1lUNkyZlvErGW1v3NdfVCF1X7KtN\nyunqFpF8oNBX3ZjZUDNbYGYrzOw9M7smtn2Amb1oZqtjXw/N27tSClnVgltb2/fan3++8wtdfXX7\nPJ7Qu1cZouuKXQbQmIBIct2+6sbMjgCOcPe3zOxgYAlwPvA3wKfufruZzQAOdfcb0r1WrlfdpLqN\n3oks5g3Gpf/hLVugoiLjMXQ1Qvh15ebrIlFQ8Ktu3P1jd38r9v0OYAUwGDgPmB1rNpsg+RdUYs33\nGc7Zd7uNpEl+7Nj2vfYkST5ZnVdXI4SfbpIhklxeBmPNrAoYAywGDnf3jyF4MwAOy8cxUtq1iyX7\nHb8vuZ/D/3Ru89578cSe4ZNDqjsVga5GCDu9GYukkE0hP90DOIigbPNXsefbOuz/LMXPTQcagIZh\nw4Z1byRiTfJlBz7db4DP+WVr5p9PIl+Drpr6Xxo679KTUIy1boA+wAvA9xO2rSKo3QMcAazK9Drd\nvupm61b38ePd/+qv3BcsSNs02wSQj5UGdfWHiBRDtok+l6tuDPg5sMLdE6eDPg1Mi30/DXiqu8fI\naMAAeP11eOIJmDAhZbNU5Zgrruhci89HnVdXf4hImORy1c0pwGvAu0DbNQ03EdTpfwUMAz4Cprr7\np+leq9Br3aS6Ysas/VUa/frBtGkwe3Zu67Po6g8RKYZsr7rp3d0DuPv/A5JPI4Uzuvu6hZDqWveO\nybi5GZ59NkjqM2cGPzdsWDCY15VB12HDkr+x6OoPESmFsl8CIZsp711JsB99lPsUcF39ISJhUtaJ\nPlXtvWOyv+22lEvYdJKPXrcWhhKRMCnr9ei7Mls1m0SvtdJFpJz0iPXou7L+TGVl8ra9eqnXLSLR\nVraJvr4+qMsnk6z8kqpuPnt212rxWgZXRMpNWSb6ttr83r2d96Ua9MxH3TzbMQERkTApyxp9qtp8\nr15BD71Q5RetYCkiYRLpGn2q2nyyHn4xjqs16UUkzMoy0ae7BLKQpRQtgysi5agsE32ygdU2hVxT\nRhOhRKQclWWibxtYTaVQpRRNhBKRclSWg7FtNDgqIj1ZpAdj26iUIiKSWVknepVSREQyK+tED7mv\nNCnSHZohLeWk2+vRi/RUbTOk225Ok3gDeXU0JIzKvkcvUmy6VaSUGyV6kS7SDGkpN0r0Il2kGdJS\nbgqW6M1skpmtMrM1ZjajUMcRKTZd1ivlpiCJ3sx6AfcB3wT+ArjIzP6iEMcSKTZd1ivlplBX3ZwA\nrHH3PwCY2VzgPGB5gY4nUlR1dUrsUj4KVboZDKxPeN4Y27aPmU03swYza2hqaipQGCIiUqhEn+xW\n3O0W1XH3We5e6+61gwYNKlAYIiJSqETfCAxNeD4E2FigY4mISBqFSvS/A0aY2XAz2x/4FvB0gY4l\nIiJpFGQw1t1bzOwq4AWgF/CQu79XiGOJiEh6oViP3syagCQry4fGQGBLqYNIQ/HlLuwxKr7chT3G\n7sRX6e4ZBzlDkejDzswaslncv1QUX+7CHqPiy13YYyxkfFoCQUQk4pToRUQiTok+O2luRR4Kii93\nYY9R8eUu7DEWLD7V6EVEIk7DIQ36AAADpUlEQVQ9ehGRiFOiT8PM1prZu2a21MwaSh0PgJk9ZGab\nzWxZwrYBZvaima2OfT00ZPHdamYbYudxqZn9ZQnjG2pmC8xshZm9Z2bXxLaH4hymiS9M57Cvmb1p\nZu/EYvyX2PbhZrY4dg4fi02WDFN8j5jZhwnnsLoU8SXE2cvM3jaz38SeF+z8KdFn9jV3rw7RZVmP\nAJM6bJsBvOzuI4CXY89L5RE6xwdwd+w8Vrv7s0WOKVEL8E/ufjQwDrgytoR2WM5hqvggPOdwF/B1\ndz8OqAYmmdk44I5YjCOAz4BLQxYfwHUJ53BpieJrcw2wIuF5wc6fEn2ZcfdXgU87bD4PmB37fjZw\nflGDSpAivtBw94/d/a3Y9zsI/tAGE5JzmCa+0PDAztjTPrGHA18Hfh3bXspzmCq+0DCzIcDZwH/F\nnhsFPH9K9Ok5MN/MlpjZ9FIHk8bh7v4xBIkCOKzE8SRzlZn9PlbaKVlpKZGZVQFjgMWE8Bx2iA9C\ndA5jZYelwGbgReADYJu7t8SadFqavJTxuXvbObwtdg7vNrMDShUfcA9wPdAae15BAc+fEn16J7t7\nDcGdsq40s9NKHVCZegD4c4KP0R8D/1HacMDMDgKeAP7R3f9Y6ng6ShJfqM6hu+9192qClWlPAI5O\n1qy4USUcuEN8ZjYKuBEYCRwPDABuKEVsZnYOsNndlyRuTtI0b+dPiT4Nd98Y+7oZmEfwHzqMNpnZ\nEQCxr5tLHE877r4p9ofXCvyMEp9HM+tDkETr3f3J2ObQnMNk8YXtHLZx923AQoLxhEPMrG2hxFAs\nTZ4Q36RYWczdfRfwMKU7hycDk81sLTCXoGRzDwU8f0r0KZhZfzM7uO174CxgWfqfKpmngWmx76cB\nT5Uwlk7aEmjMBZTwPMZqoT8HVrj7jxN2heIcpoovZOdwkJkdEvv+QGAiwVjCAmBKrFkpz2Gy+FYm\nvJEbQf27JOfQ3W909yHuXkWwhPsr7l5HAc+fJkylYGZ/RtCLh2A55/9299tKGBIAZvYoMIFgpbtN\nwC3A/wV+BQwDPgKmuntJBkRTxDeBoOTgwFrgu2318BLEdwrwGvAu8froTQR18JKfwzTxXUR4zuFo\ngsHCXgSdxV+5+w9jfzNzCcoibwPfifWewxLfK8AggjLJUuAfEgZtS8LMJgA/cPdzCnn+lOhFRCJO\npRsRkYhTohcRiTglehGRiFOiFxGJOCV6EZGIU6IXEYk4JXoRkYhTohcRibj/DzMUlxxsxRS1AAAA\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1211495f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the results\n",
    "X, Y = data['X'], data['Y']\n",
    "plt.plot(X, Y, 'bo', label='Real data')\n",
    "plt.plot(X, X * w + b, 'r', label='Predicted data')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What was my batch size? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The length of our data, so essentially there was not one. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's look at our linear model in TensorBoard\n",
    "\n",
    "### Now go to your terminal window.\n",
    "\n",
    "### Enter this code into the command line.  `tensorboard --logdir=\"./graphs/lin_reg\" --port 6006`\n",
    "\n",
    "\n",
    "### Then navigate to this url. `http://0.0.0.0:6006`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Any Questions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we have grasp of how to build a simple linear model in TensorFlow, we are going to stack all of these skills together to build our first neural network with TensorFlow. \n",
    "\n",
    "## I am going to run through this neural network example using the same process as before, but with only a couple more steps "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1: Build our graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1: Import our packages and data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-07T21:24:47.709772Z",
     "start_time": "2017-11-07T21:24:37.712829Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets('MNIST_Data', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-07T21:25:27.242907Z",
     "start_time": "2017-11-07T21:25:27.218937Z"
    }
   },
   "outputs": [],
   "source": [
    "mnist.train.images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-07T21:28:09.712393Z",
     "start_time": "2017-11-07T21:28:09.707921Z"
    }
   },
   "outputs": [],
   "source": [
    "mnist.train.labels[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-07T21:28:37.015861Z",
     "start_time": "2017-11-07T21:28:37.011582Z"
    }
   },
   "outputs": [],
   "source": [
    "mnist.train.images[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### These are the shapes of our values in training data. What does this tell us? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2: Define our Neural Network Constants**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-07T21:39:33.380414Z",
     "start_time": "2017-11-07T21:39:33.374215Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These are the number of nodes we want to have in each layer,\n",
    "# we can set these values to whatever we want.\n",
    "num_nodes_hl1 = 500\n",
    "num_nodes_hl2 = 500\n",
    "num_nodes_hl3 = 500\n",
    "\n",
    "# Number of output classes equals 10\n",
    "# Our classes are also in one hot encoding\n",
    "n_classes = 10\n",
    "\n",
    "# The size of our imput data is 784\n",
    "input_dim = 784\n",
    "\n",
    "# We can set our batch size to whatever we can fit into memory\n",
    "batch_size = 64\n",
    "\n",
    "# Determine how many epochs we would like to use\n",
    "hm_epoch = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3: Define our placeholders**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-07T21:39:36.052123Z",
     "start_time": "2017-11-07T21:39:36.045868Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set the height and wide of model\n",
    "# These are x and y variables and they must be one dimensional each\n",
    "# So the first dimension always equals None\n",
    "# The second must equal the dimension of your X and y variable\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=[None, input_dim], name='x_placeholder')\n",
    "y = tf.placeholder(tf.float32, shape=[None, n_classes], name='y_placeholder')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4: Define our variables**\n",
    "- This step will look a bit different now that we are working with a neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we just needed to define two variables our bias and our weights. However, now we have a bias and weight for every node on each layer. \n",
    "\n",
    "We need to build a dictionary of weights and biases at every layer with the correct shape to make sure we have a weight for every node. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-07T21:47:57.036024Z",
     "start_time": "2017-11-07T21:47:56.954016Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize hidden layer's variables (Inputs, Weights, and Biases) and take random sample from batch training data\n",
    "\n",
    "# Shape equals input data, and number of nodes\n",
    "hidden_layer_1 = {'weights': tf.Variable(tf.random_normal(shape=[input_dim, num_nodes_hl1], name='hidden_layer_1')),\n",
    "                  'biases': tf.Variable(tf.random_normal(shape=[num_nodes_hl1]))}\n",
    "\n",
    "# The output shape of hidden_layer_1 is the num_nodes_hl1, so that is the input for the next layer\n",
    "hidden_layer_2 = {'weights': tf.Variable(tf.random_normal(shape=[num_nodes_hl1, num_nodes_hl2], name='hidden_layer_2')),\n",
    "                  'biases': tf.Variable(tf.random_normal(shape=[num_nodes_hl2]))}\n",
    "\n",
    "\n",
    "hidden_layer_3 = {'weights': tf.Variable(tf.random_normal(shape=[num_nodes_hl2, num_nodes_hl3], name='hidden_layer_3')),\n",
    "                  'biases': tf.Variable(tf.random_normal(shape=[num_nodes_hl3]))}\n",
    "\n",
    "\n",
    "# Lastly we need the output layer, which must have the same output shape as our number of classes or our y shape\n",
    "output_layer = {'weights': tf.Variable(tf.random_normal(shape=[num_nodes_hl3, n_classes], name='output_layer')),\n",
    "                  'biases': tf.Variable(tf.random_normal(shape=[n_classes]))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There is a lot going on here. Let's break it down a little bit. \n",
    "\n",
    "- First I initialize my variable much like I did in the linear regression.\n",
    "- Second I need to assign these variables a starting value, the starting value does not really matter that much because it is going to change as my model learns. So I use `tf.random_normal()` to generate random values for my variables to start at. \n",
    "- The most important thing to noice is the shape of my variables. They are the same shape as my input to that layer and my output for that layer. I start with my X data input dimensions, and end with my target variables output dimension. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://www.pyimagesearch.com/wp-content/uploads/2016/08/simple_neural_network_header.jpg?width=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 5: Build our model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again this step is very similar to the linear model we built (Y = X * w + b), but it is going to be slightly different much like the last step. We need to account for layers sizes and weights and biases at each node. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-07T21:51:44.147873Z",
     "start_time": "2017-11-07T21:51:44.126463Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build models function  (input * weights) + biases\n",
    "# input = data\n",
    "# weights = hidden_layer_n['weights']\n",
    "# biases = hidden_layer_n['biases']\n",
    "\n",
    "l1 = tf.add(tf.matmul(x, hidden_layer_1['weights']), hidden_layer_1['biases'])\n",
    "\n",
    "# Add activation function to run this data\n",
    "l1 = tf.nn.relu(l1, name='relu_l1')\n",
    "\n",
    "l2 = tf.add(tf.matmul(l1, hidden_layer_2['weights']), hidden_layer_2['biases'])\n",
    "l2 = tf.nn.relu(l2, name='relu_l2')\n",
    "\n",
    "l3 = tf.add(tf.matmul(l2, hidden_layer_3['weights']), hidden_layer_3['biases'])\n",
    "l3 = tf.nn.relu(l3, name='relu_l3')\n",
    "\n",
    "y_predicted = tf.add(tf.matmul(l3, output_layer['weights']), output_layer['biases'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the same Y = x * w + b, but we need to do matrix multiplication instead of normal multiplication because our weights are matrices, then we use the relu function to help us make our predictions. The layer values trickle down towards the bottom until they finally get to our output layer with our final prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 6: Define our loss**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-07T21:54:39.102869Z",
     "start_time": "2017-11-07T21:54:39.060709Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The same reduce mean function but we also use softmax, a mulitinomial logistical classifier.\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_predicted, labels=y, name='loss'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If we have time I will go over this in class, if not, then I will slack out an info doc I wrote up explaining how softmax cross entropy works. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 7: Build our optimizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-07T21:55:28.367550Z",
     "start_time": "2017-11-07T21:55:28.141493Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(0.001).minimize(loss, name='optimizer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is very similar to gradient descent as an optimizer just a different flavor. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 8: Initialize all of our variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-07T21:55:34.675356Z",
     "start_time": "2017-11-07T21:55:34.671702Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize all global variables\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2: Execute our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 9: Initialize our session using a with block**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-07T21:58:25.452877Z",
     "start_time": "2017-11-07T21:57:18.986294Z"
    }
   },
   "outputs": [],
   "source": [
    "# Activate our sessions and repeat over for loop\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # compile all the tf assigned variables (tf.Variables)\n",
    "    sess.run(init)\n",
    "\n",
    "    # Initialize our TensorBoard writer for our NN\n",
    "    writer = tf.summary.FileWriter('./graphs/nn', sess.graph)\n",
    "\n",
    "    # Loop over the defined number of epochs\n",
    "    for epoch in range(hm_epoch):\n",
    "        # initialize loss at 0\n",
    "        total_loss = 0\n",
    "\n",
    "        # Initialize for loop for batches\n",
    "        for _ in range(int(mnist.train.num_examples/batch_size)):\n",
    "\n",
    "            # Tensor flow's mnist next batch function breaks your training data up for you based on your batch size\n",
    "            epoch_x, epoch_y = mnist.train.next_batch(batch_size)\n",
    "\n",
    "            # we use _ because it returns a tuple where the first value is always none, so we dont care about it\n",
    "            _, l = sess.run([optimizer, loss], feed_dict= {x: epoch_x, y: epoch_y})\n",
    "\n",
    "            # add loss to epoch loss\n",
    "            total_loss += l\n",
    "\n",
    "        # Print out our total_loss at each Epoch\n",
    "        print('Epoch ', epoch, ' completed out of ', hm_epoch, ', loss: ', total_loss)\n",
    "        \n",
    "    # close the writer when you're done using it\n",
    "    writer.close()\n",
    "\n",
    "\n",
    "    # Determine the correct amount of classifications\n",
    "    correct = tf.equal(tf.argmax(y_predicted, 1), tf.argmax(y, 1))\n",
    "    \n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
    "    print('Accuracy ', accuracy.eval({x:mnist.test.images, y: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's look at our linear model in TensorBoard\n",
    "\n",
    "### Now go to your terminal window.\n",
    "\n",
    "### Enter this code into the command line.  \n",
    "`tensorboard --logdir=\"./graphs/nn\" --port 6006`\n",
    "\n",
    "\n",
    "### Then navigate to this url. \n",
    "`http://0.0.0.0:6006`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ooooo... Yikes it looks like garbage. I can barely interpret that.\n",
    "\n",
    "\n",
    "### Well they thought of this. We need to place the these variables into hierarchical blocks.\n",
    "\n",
    "\n",
    "### We do this by using `with tf.name_scope('loss')` for instance.\n",
    "\n",
    "\n",
    "### Look at my example below. Restart your kernel before running."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Networks are typically made as python scripts. I do not usually use jupyter notebooks because it is harder to keep track of all the variables. The full python script is below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-07T17:36:28.091589Z",
     "start_time": "2017-11-07T17:35:28.915303Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets('MNIST_Data', one_hot=True)\n",
    "\n",
    "# These are the number of nodes we want to have in each layer,\n",
    "# we can set these values to whatever we want.\n",
    "num_nodes_hl1 = 500\n",
    "num_nodes_hl2 = 500\n",
    "num_nodes_hl3 = 500\n",
    "\n",
    "# Number of output classes equals 10\n",
    "# Our classes are also in one hot encoding\n",
    "n_classes = 10\n",
    "\n",
    "# The size of our imput data is 784\n",
    "input_dim = 784\n",
    "\n",
    "# We can set our batch size to whatever we can fit into memory\n",
    "batch_size = 100\n",
    "\n",
    "# Determine how many epochs we would like to use\n",
    "hm_epoch = 10\n",
    "\n",
    "# Set the height and wide of model\n",
    "# These are x and y variables and they must be one dimensional each\n",
    "# So the first dimension always equals None\n",
    "# The second must equal the dimension of your X and y variable\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=[None, input_dim], name='x_placeholder')\n",
    "y = tf.placeholder(tf.float32, shape=[None, n_classes], name='y_placeholder')\n",
    "\n",
    "# Initialize hidden layer's variables (Inputs, Weights, and Biases) and take random sample from batch training data\n",
    "\n",
    "\n",
    "# Shape equals input data, and number of nodes\n",
    "hidden_layer_1 = {'weights': tf.Variable(tf.random_normal(shape=[input_dim, num_nodes_hl1], name='hidden_layer_1')),\n",
    "                  'biases': tf.Variable(tf.random_normal(shape=[num_nodes_hl1]))}\n",
    "\n",
    "# The output shape of hidden_layer_1 is the num_nodes_hl1, so that is the input for the next layer\n",
    "hidden_layer_2 = {'weights': tf.Variable(tf.random_normal(shape=[num_nodes_hl1, num_nodes_hl2], name='hidden_layer_2')),\n",
    "                  'biases': tf.Variable(tf.random_normal(shape=[num_nodes_hl2]))}\n",
    "\n",
    "hidden_layer_3 = {'weights': tf.Variable(tf.random_normal(shape=[num_nodes_hl2, num_nodes_hl3], name='hidden_layer_3')),\n",
    "                  'biases': tf.Variable(tf.random_normal(shape=[num_nodes_hl3]))}\n",
    "\n",
    "# Lastly we need the output layer, which must have the same output shape as our number of classes or our y shape\n",
    "output_layer = {'weights': tf.Variable(tf.random_normal(shape=[num_nodes_hl3, n_classes], name='output_layer')),\n",
    "                  'biases': tf.Variable(tf.random_normal(shape=[n_classes]))}\n",
    "\n",
    "with tf.name_scope('weights'):\n",
    "    hidden_layer_1['weights']\n",
    "    hidden_layer_2['weights']\n",
    "    hidden_layer_3['weights']\n",
    "    output_layer['weights']\n",
    "\n",
    "with tf.name_scope('biases'):\n",
    "    hidden_layer_1['biases']\n",
    "    hidden_layer_2['biases']\n",
    "    hidden_layer_3['biases']\n",
    "    output_layer['biases']\n",
    "\n",
    "    \n",
    "# Build models function  (input * weights) + biases\n",
    "# input = data\n",
    "# weights = hidden_layer_n['weights']\n",
    "# biases = hidden_layer_n['biases']\n",
    "with tf.name_scope('ReLU'):\n",
    "    l1 = tf.add(tf.matmul(x, hidden_layer_1['weights']), hidden_layer_1['biases'])\n",
    "\n",
    "    # Add activation function to run this data\n",
    "    l1 = tf.nn.relu(l1, name='relu_l1')\n",
    "\n",
    "    l2 = tf.add(tf.matmul(l1, hidden_layer_2['weights']), hidden_layer_2['biases'])\n",
    "    l2 = tf.nn.relu(l2, name='relu_l2')\n",
    "\n",
    "    l3 = tf.add(tf.matmul(l2, hidden_layer_3['weights']), hidden_layer_3['biases'])\n",
    "    l3 = tf.nn.relu(l3, name='relu_l3')\n",
    "\n",
    "    y_predicted = tf.add(tf.matmul(l3, output_layer['weights']), output_layer['biases'])\n",
    "\n",
    "# The same reduce mean function but we also use softmax, a mulitinomial logistical classifier.\n",
    "with tf.name_scope('loss'):\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_predicted, labels=y, name='loss'))\n",
    "    \n",
    "with tf.name_scope('train'):\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(loss, name='optimizer')\n",
    "    \n",
    "with tf.name_scope('Accuracy'):\n",
    "    # Accuracy\n",
    "    correct = tf.equal(tf.argmax(y_predicted, 1), tf.argmax(y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
    "\n",
    "# create a summary for our cost and accuracy\n",
    "tf.summary.scalar(\"cost\", loss)\n",
    "tf.summary.scalar(\"accuracy\", accuracy)\n",
    "\n",
    "tf.summary.merge_all()\n",
    "\n",
    "# Initialize all global variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Activate our sessions and repeat over for loop\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # compile all the tf assigned variables (tf.Variables)\n",
    "    sess.run(init)\n",
    "\n",
    "    # Initialize our TensorBoard writer for our NN\n",
    "    writer = tf.summary.FileWriter('./graphs/new_nn', sess.graph)\n",
    "\n",
    "    # Loop over the defined number of epochs\n",
    "    for epoch in range(hm_epoch):\n",
    "        # initialize loss at 0\n",
    "        total_loss = 0\n",
    "\n",
    "        # Initialize for loop for batches\n",
    "        for _ in range(int(mnist.train.num_examples/batch_size)):\n",
    "\n",
    "            # Tensor flow's mnist next batch function breaks your training data up for you based on your batch size\n",
    "            epoch_x, epoch_y = mnist.train.next_batch(batch_size)\n",
    "\n",
    "            # we use _ because it returns a tuple where the first value is always none, so we dont care about it\n",
    "            _, l = sess.run([optimizer, loss], feed_dict= {x: epoch_x, y: epoch_y})\n",
    "\n",
    "            # add loss to epoch loss\n",
    "            total_loss += l\n",
    "\n",
    "        # Print out our total_loss at each Epoch\n",
    "        print('Epoch ', epoch, ' completed out of ', hm_epoch, ', loss: ', total_loss)\n",
    "        \n",
    "    # close the writer when you're done using it\n",
    "    writer.close()\n",
    "\n",
    "\n",
    "    # Determine the correct amount of classifications\n",
    "    \n",
    "    print('Accuracy ', accuracy.eval({x:mnist.test.images, y: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's look at our linear model in TensorBoard\n",
    "\n",
    "### Now go to your terminal window.\n",
    "\n",
    "### Enter this code into the command line.  \n",
    "`tensorboard --logdir=\"./graphs/new_nn\" --port 6006`\n",
    "\n",
    "\n",
    "### Then navigate to this url. \n",
    "`http://0.0.0.0:6006`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tip**: Do no build TensorFlow NN in jupyter notebooks, it's not very fun. jupyter notebooks does not restart the kernel per NN so everything is kept in the build graph, and will slow everything down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
